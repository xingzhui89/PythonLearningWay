{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今天的目标是通过一个练习任务来探索scikit-learn的一些主要工具，任务是分析一个20个不同主题的文本集合（newsgroups posts）。\n",
    "在这一节，我们将会学到：\n",
    "*加载文件内容和类别\n",
    "*提取适合机器学习的特征向量\n",
    "*训练一个线性模型来执行分类\n",
    "*使用一个grid search自动调参方法，来寻找对于特征提取成分和分类都不错的配置。\n",
    "\n",
    "### Loading the 20 newsgroups dataset\n",
    "\n",
    "数据集的名称是Twenty Newsgroups。这个数据集中大概有20000个新闻组文件，分为20个不同的组。一般用于文本分类和聚类的机器学习练习。\n",
    "\n",
    "接下来我们会使用内置的dataset加载器来获取20newsgroups。或者，我们也可以手动下载数据集，然后利用sklearn.datasets.load_files函数来加载，注意要未压缩的存档文件夹。\n",
    "为了加快执行速度，我们只使用4个类别，而不是全部20个类别。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以加载匹配这几个分类的列表了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回的数据集是一个scikit-learn的bunch：一个简单的具有域的holder对象，可以像python的dict对象一样访问，比如说target_names表示了请求的类别名："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件本身加载在内存中，在data属性上访问。filenames也是可以访问的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们打印加载的第一个文件的一开始的行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习算法要求在测试集中为每个document提供一个label。在这个例子中，类别就是新闻组的名称，同时也是存储特定文件的文件夹的名称。\n",
    "考虑到速度和空间的效率，scikit-learn将目标属性加载为一个整数数组，与target_names列表中的类别名称相对应。每个样本的类别整数id保存在target属性中：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可能这样获取类别名称："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能注意到，样本已经被随机洗牌了：如果我们只选择一开始的一些样品去快速训练一个模型，并且没有在整个数据集上重复训练，这样随机打乱样本的顺序是有帮助的。\n",
    "\n",
    "### Extracting features from text files\n",
    "从文本中提取特征\n",
    "\n",
    "为了执行文本文件的机器学习，我们需要将文本内容转化成数字特征向量。\n",
    "\n",
    "#### Bag of words，BOW模型，也就是词袋模型。\n",
    "最直观的方式就是用词袋模型：\n",
    "*给训练集中任何一个文件中的每一个词，都指定一个固定的数字id，或者来说是建立一个从单词到数字的词典映射。\n",
    "*对每一个文本#i，计算每个单词w在文本中出现的次数，并且保存在X[i，j]中，j表示在词典中单词w的序列号。\n",
    "\n",
    "n_features表示在语料库中不同单词的数目，这个值一般都会大于100000.\n",
    "如果n_samples==10000，存储X为一个numpy数组的话，需要RAM的4GB空间，对于先进的电脑来说不太能接受的。\n",
    "幸运的是，X中的大部分值是0，因为一个给定的文档中，并不是所有单词都能被使用到。这样来讲，我们可以认为词袋是一个典型的高维稀疏数据集。我们可以保存很多内存，只存储特征向量的非零部分。\n",
    "scipy.sparse矩阵就是这样的数据结构，scikit-learn内置了对这种结构的支持。\n",
    "\n",
    "#### Tokenizing text with scikit-learn\n",
    "用scikit-learn标记文本\n",
    "停止词的文本预处理、标记和过滤包含在一个高级组件中，该组件能够构建一个特征字典并将文档转化为特征向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer支持N-gram的单词或者连续字符的计数。一旦安装好，向量机就建立了一个特征索引字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个单词的索引值与其在整个训练集中的频率有关。\n",
    "\n",
    "#### From occurrences to frequencies\n",
    "\n",
    "发生次数的统计是一个很好的开端，但是存在一个问题：越长的文档就会有更高的平均计数值，尽快他们会讨论相同的主题。\n",
    "为了避免这些潜在的差异，将文档中每个单词的出现次数除以文档中的单词总数就可以了，得到的新特性成为术语频率，Term Frequency，简称tf。\n",
    "在tf的基础上，另一个改进是对语料库中许多文档中出现的单词进行减重，因此比只出现在语料库较小部分中的单词的信息要少。\n",
    "这个减重的操作称为tf-idf，Term Frequency times Inverse Document Frequency\n",
    "tf和tf-idf都可以这么来计算：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的范例代码中，我们首先使用fit()方法去拟合我们的预测器，然后使用transform()方法去转换我们的计数矩阵为一个tf-idf的表示。这两步可以整合来实现并加速处理，因为减少了多余处理过程。如下所示，使用fit_transform()方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier，训练一个分类器\n",
    "\n",
    "现在我们已经有了自己的特征，我们就可以训练一个分类器来尝试预测一个文章的类别。让我们从一个pusu贝叶斯分类器开始，它为这个任务提供了一个很好的基线。scikit-learn包括这个分类器的几个变体：最适合单词计数的是多项式变体：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了预测新文档的结果，我们需要使用与以前几乎相同的特征提取链来提取特征。不同之处在于，我们使用transform而不是fit_transform，因为他们已经适合于训练集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a pipeline，创建一个pipeline\n",
    "\n",
    "为了更方便的执行向量化——〉转换——〉分类的过程，scikit-learn提供了一个Pipelien类，就跟一个复合的分类器一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vect，tfidf和clf的名称是完全随意的，我们应当看到在前面网格搜索章节使用过pipeline。\n",
    "我们现在可以使用一行命令来训练模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the performance on the test set，在测试集上评估表现\n",
    "\n",
    "评估预测模型的准确度也是比较简单的：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83488681757656458"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们得到了83.4%的准确率。让我们看看，如果使用一个线性SVM（被认为是最好的文本分类算法，尽快比朴素贝叶斯要慢一些），是否会得到更好的结果。只要简单的在pipeline中更换一下分类器即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.linear_model import SGDClassifier\n",
    ">>> text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "... ('tfidf', TfidfTransformer()),\n",
    "... ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "... alpha=1e-3, random_state=42,\n",
    "... max_iter=5, tol=None)),\n",
    "... ])\n",
    ">>> text_clf.fit(twenty_train.data, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9127829560585885"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> predicted = text_clf.predict(docs_test)\n",
    ">>> np.mean(predicted == twenty_test.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn提供更详细的结果分析工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.88      0.97      0.92       389\n",
      "               sci.med       0.94      0.90      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "           avg / total       0.92      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn import metrics\n",
    ">>> print(metrics.classification_report(twenty_test.target, predicted,\n",
    "... target_names=twenty_test.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[258,  11,  15,  35],\n",
       "       [  4, 379,   3,   3],\n",
       "       [  5,  33, 355,   3],\n",
       "       [  5,  10,   4, 379]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如预期的那样，混乱矩阵显示，来自无神论和基督教新闻组的帖子常常彼此混淆，而计算机图形组的会较少。\n",
    "\n",
    "### Parameter tuning using grid search\n",
    "使用网格搜索算法对参数进行调节\n",
    "\n",
    "我们已经在TfidfTransformer中遇到一些参数，类似于use_idf。分类器应该有很多的参数，类似于：MultinomialNB包括一个平滑参数alpha，而SGDClassifier在目标函数中有一个惩罚参数alpha和一个可配置损失和惩罚项。\n",
    "\n",
    "与其调整chain中各个组件的参数，还不如在可能值得网格上对最佳参数进行彻底搜索。我们在单词或双体字上使用所有分类器，无论是否使用idf，对SVM的惩罚参数设置为0.01或者0.001：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "... 'tfidf__use_idf': (True, False),\n",
    "... 'clf__alpha': (1e-2, 1e-3),\n",
    "... }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明显的，这样彻底的搜索代价是比较昂贵的。如果我们有多个CPU可供分配，我们可以告诉grid search，尝试这个八个参数的组合。如果我们设置n_jobs参数为-1，那么grid search会自动检测CPU："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search实例表现的像一个普通的scikit-learn模型。让我们在一个较小的子集上运行这个search，加快计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到一个分类器，我们可以用来预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对象的best_score_和best_params_属性存储了最佳的平均分数和对应的参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search的更详细的总结可以通过.cv_results_来获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
