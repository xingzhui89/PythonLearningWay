{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: grouping observations together\n",
    "聚类，将观测数据分类\n",
    "\n",
    "聚类可以解决的问题举例：\n",
    "还是之前的iris数据集，如我们已经知道有3种类型的iris花，但是并没有方法对数据集中的样品进行分类：这时候我们可以尝试聚类的方法，将所有的观测数据分割为具有较明显特征的群组中。\n",
    "\n",
    "### K-means clustering\n",
    "目前存在很多不同的聚类标准和关联算法。最简单的聚类算法是K-means。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster,datasets\n",
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris=iris.data\n",
    "y_iris=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means=cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means.labels_[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_iris[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "警告：算法并不能保证绝对分类正确。首先，确定数据样本有多少个种类很难。其次，算法对于初始化的十分敏感的，可能陷入局部最小值陷阱，尽管scikit-learn采用了一些方法来减轻这些问题的影响。\n",
    "\n",
    "所以说，不要过度解读聚类的结果~\n",
    "\n",
    "#### 应用实例：vector quantization，矢量量化？\n",
    "一般来说，使用聚类和Kmeans方法，可以视为选取较少的样本来压缩信息量的一种方式。这种问题有时称作vector quantization。举例来说，可以应用到对图片进行色调分离。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "try:\n",
    "    face=sp.face(gray=True)\n",
    "except AttributeError:\n",
    "    from scipy import misc\n",
    "    face=misc.face(gray=True)\n",
    "X=face.reshape((-1,1))\n",
    "k_means=cluster.KMeans(n_clusters=5,n_init=1)\n",
    "k_means.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "values=k_means.cluster_centers_.squeeze()\n",
    "labels=k_means.labels_\n",
    "face_compressed=np.choose(labels,values)\n",
    "face_compressed.shape=face.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical agglomerative clustering: Ward\n",
    "分层聚类\n",
    "\n",
    "分层聚类方法是一种聚类分析，旨在构建聚类的层次结构。一般来说，这种技术的一些方法包括：\n",
    "1、Agglomerative—凝聚—bottom-up由下而上的方法：每个样本始于它本身的聚类，并且聚类以最小化一个linkage标准的方法迭代合并。当仅有较少的观测数据的时候，这种方法尤其有趣。当聚类数量较大的时候，它比k-means在计算上更有效率。\n",
    "2、Divisive—分裂—top-down由上而下的方法：所有的观测数据从一个聚类出发，然后不断向下迭代分裂。当预测较多数量的聚类时，这种方法又慢，统计上来讲也是病态的。\n",
    "\n",
    "Connectivity-constrained clustering\n",
    "使用Agglomerative聚类方法，可以通过给出连通性图来指定哪些样本可以聚集在一起。scikit中的图由它们的邻接矩阵表示。经常，使用稀疏性矩阵。例如，这可以用于在对图像进行聚类时检索连接区域。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\py35\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Generate data\n",
    "try:  # SciPy >= 0.16 have face in misc\n",
    "    from scipy.misc import face\n",
    "    face = face(gray=True)\n",
    "except ImportError:\n",
    "    face = sp.face(gray=True)\n",
    "\n",
    "# Resize it to 10% of the original size to speed up the processing\n",
    "face = sp.misc.imresize(face, 0.10) / 255.\n",
    "\n",
    "X = np.reshape(face, (-1, 1))\n",
    "\n",
    "# Define the structure A of the data. Pixels connected to their neighbors.\n",
    "connectivity = grid_to_graph(*face.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature agglomeration\n",
    "我们已经知道，稀疏性可以减轻维度诅咒，比如，与特征的数量相比，观察量不足。另一个方法是合并类似的特征：feature agglomeration。这个方法可以通过在特征方向的聚类来实现，也就是说转置数据后再聚类。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',\n",
       "           connectivity=<64x64 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 288 stored elements in COOrdinate format>,\n",
       "           linkage='ward', memory=None, n_clusters=32,\n",
       "           pooling_func=<function mean at 0x000002A1A73A5F28>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits=datasets.load_digits()\n",
    "images=digits.images\n",
    "X=np.reshape(images,(len(images),-1))\n",
    "connectivity=grid_to_graph(*images[0].shape)\n",
    "agglo=cluster.FeatureAgglomeration(connectivity=connectivity,n_clusters=32)\n",
    "agglo.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced=agglo.transform(X)\n",
    "X_approx=agglo.inverse_transform(X_reduced)\n",
    "images_approx=np.reshape(X_approx,images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompositions: from a signal to components and loadings\n",
    "分解：从一个信号到组件和装载\n",
    "\n",
    "如果X是我们的多元数据，那么我们要去解决的问题即为，在一个不同观测角度重写X：我们想学习到loading L和一系列成分C，满足X=LC。目前存在的不同用于选择成分的标准。\n",
    "\n",
    "### Principal component analysis: PCA\n",
    "主成分分析\n",
    "PCA选择信号中可以解释最大方法的连续成分变量。\n",
    "\n",
    "上述观察所涵盖的点云在一个方向上非常平坦：三个单变量特征之一几乎可以使用另外两个特征精确计算。PCA找到数据不平坦的方向。\n",
    "当用于变换数据时，PCA可以通过在主子空间上投影来降低数据的维数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> # Create a signal with only 2 useful dimensions\n",
    ">>> x1 = np.random.normal(size=100)\n",
    ">>> x2 = np.random.normal(size=100)\n",
    ">>> x3 = x1 + x2\n",
    ">>> X = np.c_[x1, x2, x3]\n",
    "\n",
    ">>> from sklearn import decomposition\n",
    ">>> pca = decomposition.PCA()\n",
    ">>> pca.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.01497565e+00   1.10035180e+00   7.57434866e-32]\n"
     ]
    }
   ],
   "source": [
    ">>> print(pca.explained_variance_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> # As we can see, only the 2 first components are useful\n",
    ">>> pca.n_components = 2\n",
    ">>> X_reduced = pca.fit_transform(X)\n",
    ">>> X_reduced.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis: ICA\n",
    "独立成分分析\n",
    "\n",
    "ICA选择的成分，使得它们的Loading分配携带最大量的独立信息\n",
    "ICA可以恢复非高斯分布的独立信号。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> # Generate sample data\n",
    ">>> import numpy as np\n",
    ">>> from scipy import signal\n",
    ">>> time = np.linspace(0, 10, 2000)\n",
    ">>> s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    ">>> s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n",
    ">>> s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n",
    ">>> S = np.c_[s1, s2, s3]\n",
    ">>> S += 0.2 * np.random.normal(size=S.shape)  # Add noise\n",
    ">>> S /= S.std(axis=0)  # Standardize data\n",
    ">>> # Mix data\n",
    ">>> A = np.array([[1, 1, 1], [0.5, 2, 1], [1.5, 1, 2]])  # Mixing matrix\n",
    ">>> X = np.dot(S, A.T)  # Generate observations\n",
    "\n",
    ">>> # Compute ICA\n",
    ">>> ica = decomposition.FastICA()\n",
    ">>> S_ = ica.fit_transform(X)  # Get the estimated sources\n",
    ">>> A_ = ica.mixing_.T\n",
    ">>> np.allclose(X,  np.dot(S_, A_) + ica.mean_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
